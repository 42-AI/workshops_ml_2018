{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes for Spam Classification\n",
    "\n",
    "\n",
    "Naive bayes is a relatively simple probabilistic classfication algorithm that is well suitable for categorical data .\n",
    "\n",
    "In machine learning, common application of Naive Bayes are spam email classification, sentiment analysis, document categorization. Naive bayes is advantageous over other commonly used classification algorithms in its simplicity, speed, and its accuracy on small data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "We will be using a data from the UCI machine learning repository that countains several Youtube comments from very popular music videos. Each comment in the data has been labeled as either spam or ham (legitimate comment), we will use this data to train our Naive Bayes algorithm for youtube comment spam classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "# For data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# For matrix operations\n",
    "import numpy as np\n",
    "\n",
    "# For regex\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data from the 'YoutubeCommentsSpam.csv' file using pandas\n",
    "data_comments = \n",
    "\n",
    "# Create column labels: 'content' and 'label'. \n",
    "# tips: the 'colums' method can be of help \n",
    "\n",
    "\n",
    "# display the first rows of our dataset to make sure that the labels have been added\n",
    "data_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{WARNING: DO NOT check the links in the spam comments! ;)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show spam comments in data\n",
    "# DO NOT GO ON THE LINKS BELOW!!! seriously, they're spams... \n",
    "print(data_comments[\"content\"][data_comments[\"label\"] == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Browsing over the comments that have been labeled as spam in this data, it seems like these comments are either unrelated to the video, or are some form of advertisement. The phrase \"check out\" seems to be very popular in this comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics and Data Cleaning\n",
    "\n",
    "The table below shows that this data set consist of $1959$ youtube comments, about $49\\%$ of them are legitimate comments and about $51\\%$ are spam. This high variation of classes in our data set will help us test our algorithms accuracy on the test data set. The average length of each comment is about $96$ characters, which is roughly about $15$ words on average per comment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add another column with corresponding comment length\n",
    "# tips: use map and lambda\n",
    "data_comments['length'] = \n",
    "\n",
    "# Display summary statistics (mean, stdev, min, max)\n",
    "data_comments[[\"label\",\"length\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of evaluation our Naive Bayes classification algorithm, we will split the data into a training and test set. The training set will be used to train the spam classification algorithm, and the test set will only be used to test its accuracy. In general the training set should be bigger than the test set and both have should be drawn from the same population (population in our case is youtube comments for music videos). We will randomly select $75\\%$ of the data as training, and $25\\%$ of the data for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's split data into training and test set (75% training, 25% test)\n",
    "\n",
    "# Set seed so we get same random allocation on each run of code\n",
    "np.random.seed(2017)\n",
    "\n",
    "# Add column vector 'uniform' of randomly generated numbers from 0 to 1 \n",
    "# tips: in numpy there is a method to draw sample from a uniform distribution\n",
    "data_comments[\"uniform\"] = \n",
    "\n",
    "# As the number in our 'uniform' column is uniformly distributed, \n",
    "# about 75% of these numbers should be less than 0.75, let's grab those 75%\n",
    "data_comments_train = \n",
    "\n",
    "# same for the 25% of these numbers should that are greater than 0.75\n",
    "data_comments_test = \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that training data have both spam and ham comments\n",
    "data_comments_train[\"label\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for the test data \n",
    "data_comments_test[\"label\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the training and test data have a good mix spam and ham comments, so we are ready to move onto training the Naive Bayes classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all the comments into a big list\n",
    "# tips: list.join()\n",
    "training_list_words = \n",
    "\n",
    "# Split the list of comments into a list of unique words\n",
    "# tips: set() \n",
    "train_unique_words = \n",
    "\n",
    "# Number of unique words in training \n",
    "vocab_size_train = \n",
    "\n",
    "# Description of summarized comments in training data\n",
    "print('Unique words in training data: %s' % vocab_size_train)\n",
    "print('First 5 words in our unique set of words: \\n % s' % list(train_unique_words)[1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should look like somthing like:\n",
    "\n",
    "```Unique words in training data: 5898\n",
    "First 5 words in our unique set of words: \n",
    "['now!!!!!!', 'yellow', 'four', '/>.Pewdiepie', 'Does']```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently \"now!!\" and \"now!!!!\", as well as \"DOES\",\"DoEs\", and \"does\" are all considered to be unique words. For the purposes of spam classification, its probably better to process the data slightly to increase accuracy. In our case we can focus on letters and numbers, as well as convert all the comments to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep letters and numbers\n",
    "# tips: use regex and list comprehension\n",
    "train_unique_words = \n",
    "\n",
    "# Convert to lower case and get unique set of words\n",
    "# tips: set() ?\n",
    "train_unique_words = \n",
    "\n",
    "# Number of unique words in training \n",
    "vocab_size_train = \n",
    "\n",
    "# Description of summarized comments in training data\n",
    "print('Unique words in processed training data: %s' % vocab_size_train)\n",
    "print('First 5 words in our processed unique set of words: \\n % s' % list(train_unique_words)[1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes for Spam Classification\n",
    "\n",
    "ok, so here's the deal:\n",
    "\n",
    "- first we are going to separate our training data into 2 subsets: train and test\n",
    "\n",
    "- then create several functions to check how many time each word apparear in spam and not spam comments, check the probability of each word appearing in spam/not spam\n",
    "\n",
    "- then the 2 most important function: train() and classify()\n",
    "\n",
    "- And finally check the accurracy of our predictions\n",
    "\n",
    "Let's code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with comment words as \"keys\", and their label as \"value\"\n",
    "trainPositive = dict()\n",
    "trainNegative = dict()\n",
    "\n",
    "# Intiailize classes to zero\n",
    "positiveTotal = \n",
    "negativeTotal = \n",
    "\n",
    "# Initialize Prob. of to zero, but float ;) \n",
    "pSpam = \n",
    "pNotSpam = \n",
    "\n",
    "# Laplace smoothing\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def initialize_dicts():\n",
    "\n",
    "# Initialize dictionary of words and their labels   \n",
    "for word in train_unique_words:\n",
    "    \n",
    "    # Classify all words for now as ham (legitimate)\n",
    "    trainPositive[word] = 0\n",
    "    trainNegative[word] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of times word in comment appear in spam and ham comments\n",
    "def processComment(comment,label):\n",
    "    global positiveTotal\n",
    "    global negativeTotal\n",
    "    \n",
    "    # Split comments into words\n",
    "    comment = \n",
    "    \n",
    "    # Go over each word in comment\n",
    "    for ...\n",
    "        \n",
    "        # check if comment is not spam\n",
    "        \n",
    "            \n",
    "            # Increment number of times word appears in not spam comments\n",
    "            \n",
    "            \n",
    "        # spam comments\n",
    "        \n",
    "            \n",
    "            # Increment number of times word appears in spam comments\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Prob(word|spam) and Prob(word|ham)\n",
    "def conditionalWord(word,label):\n",
    "   \n",
    "    # Laplace smoothing parameter\n",
    "    # remider: to have access to a global variable inside a function \n",
    "    # you have to specify it using the word 'global'\n",
    "    \n",
    "    \n",
    "    # word in ham comment\n",
    "    if(label == 0):\n",
    "        # Compute Prob(word|ham)\n",
    "        \n",
    "    \n",
    "    # word in spam comment\n",
    "    else:\n",
    "        \n",
    "        # Compute Prob(word|ham)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Prob(spam|comment) or Prob(ham|comment)\n",
    "def conditionalComment(comment,label):\n",
    "    \n",
    "    # Initialize conditional probability\n",
    "    prob_label_comment = 1.0\n",
    "    \n",
    "    # Split comments into list of words\n",
    "    \n",
    "    \n",
    "    # Go through all words in comments\n",
    "    for ...\n",
    "        \n",
    "        # Compute value proportional to P(label|comment)\n",
    "        # Conditional indepdence is assumed here\n",
    "        \n",
    "    \n",
    "    return prob_label_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train naive bayes by computing several conditional probabilities in training data\n",
    "def train():\n",
    "    # reminder: we will need pSpam and pNotSpam here ;) \n",
    "\n",
    "\n",
    "    # Initiailize our variables: the total number of comment and the number of spam comments \n",
    "\n",
    "    \n",
    "    # Go over each comment in training data \n",
    "    for ...\n",
    "        \n",
    "       # check if comment is spam or not \n",
    "    \n",
    "       # increment the values depending if comment is spam or not\n",
    "        \n",
    "       # update dictionary of spam and not spam comments\n",
    "    \n",
    "    \n",
    "    # Compute prior probabilities, P(spam), P(ham)\n",
    "    pSpam = \n",
    "    pNotSpam = \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run naive bayes\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify comment are spam or ham\n",
    "def classify(comment):\n",
    "    \n",
    "    # get global variables\n",
    "    \n",
    "    \n",
    "    # Compute value proportional to Pr(comment|ham)\n",
    "    isNegative = \n",
    "    \n",
    "    # Compute value proportional to Pr(comment|spam)\n",
    "    isPositive = \n",
    "    \n",
    "    # Output True = spam, False = ham depending of the 2 previously compute variables\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spam prediction in test data\n",
    "prediction_test = []\n",
    "\n",
    "# Get prediction accuracy on test data\n",
    "for ...\n",
    "\n",
    "    # add classified comment to prediction_test list \n",
    "    \n",
    "\n",
    "# Check accuracy: \n",
    "# first the number of correct prediction \n",
    "correct_labels = \n",
    "# then the mean of correct predictions\n",
    "test_accuracy = \n",
    "\n",
    "#print prediction_test\n",
    "print(\"Proportion of comments classified correctly on test set: %s\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to write some comments to see whether they are classified as spam or ham. Recall the \"True\" is for spam comments, and \"False\" is for ham comments. \n",
    "Try your own !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spam\n",
    "classify(\"Guys check out my new chanell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spam\n",
    "classify(\"I have solved P vs. NP, check my video https://www.youtube.com/watch?v=dQw4w9WgXcQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ham\n",
    "classify(\"I liked the video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ham\n",
    "classify(\"Its great that this video has so many views\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to go further...\n",
    "## Extending Bag of Words by Using TF-IDF\n",
    "\n",
    "So far we have been using the Bag of Words model to represent comments as vectors. The \"Bag of Words\" is a list of all unique words found in the training data, then each comment can be represented by a vector that contains the frequency of each unique word that appeared in the comment. For example if the training data contains the words $(hi, how, my, grade, are, you),$ then the text \"how are you you\" can be represented by $(0,1,0,0,1,2).$ The main reason we do this in our application is because comments can vary in length, but the length of all unique words stays fixed. \n",
    "\n",
    "In our context, the TF-IDF is a measure of how important a word is in a comment relative to all the words in our training data. For example if a word such as \"the\" appeared in most of the comments, the TF-IDF would be small as this word does not help us differentiate accross comments. Note that \"TF\" stands for \"Term Frequency\", and \"IDF\" stands for \"Inverse Document Frequency\". In particular, \"TF\" denoted by $tf(w,c)$ is the number of times the word $w$ appears in the given comment $c$. Whereas \"IDF\" is a measure of how much information a given word provides in differentiating comments. Specefically, $IDF$ is formulated as $idf(w, D) = log(\\frac{\\text{Number of comments in train data $D$}}{\\text{Number of comments containing the word $w$}}).$ To combine \"TF\" and \"IDF\" together, we simple take the product, hence $$TFIDF = tf(w,c) \\times idf(w, D) = (\\text{Number of times $w$ appears in comment $c$})\\times log(\\frac{\\text{Number of comments in train data $D$}}{\\text{Number of comments containing the word $w$}}).$$\n",
    "Now the $TF-IDF$ can be used to weight the vectors that result from the \"Bag of Words\" approach. For example, suppose a comment contains \"this\" 2 times, hence $tf = 2$. If we then had 1000 comments in our traininig data, and the word \"this\" appears in 100 comments, $idf = log(1000/100) = 2.$ Therefore in this example, the TF-IDF weight would be 2*2 = 4 for the word \"this\" appear twice in a particular comment. To incorprate TF-IDF into the naive bayes setting, we can compute $$Pr(word|spam) = \\frac{\\sum_{\\text{c is spam}}TFIDF(word,c,D)}{\\sum_{\\text{word in spam c}}\\sum_{\\text{c is spam}}TFIDF(word,c,D)+ \\text{Number of unique words in data}},$$ where $TFIDF(word,c,D) = TF(word,c) \\times IDF(word,data).$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute tfidf(word, comment, data)\n",
    "def TFIDF(comment, train):\n",
    "    \n",
    "    # Split comment into list of words\n",
    "    comment = \n",
    "    \n",
    "    # Initiailize tf-idf for given comment\n",
    "    tfidf_comment = \n",
    "    \n",
    "    # Initiailize number of comments containing a word\n",
    "    num_comment_word = 0\n",
    "    \n",
    "    # Intialize index for words in comment\n",
    "    word_index = 0\n",
    "    \n",
    "    # Go over all words in comment\n",
    "    for...\n",
    "        \n",
    "        # Compute term frequence (tf)\n",
    "        # Count frequency of word in comment\n",
    "        tf = \n",
    "        \n",
    "        # Find number of comments containing word\n",
    "        for ...\n",
    "            \n",
    "            # Increment word counter if word found in comment\n",
    "            if ...\n",
    "        \n",
    "        # Compute inverse document frequency (idf)\n",
    "        # log(Total number of comments/number of comments with word)\n",
    "        idf = \n",
    "        \n",
    "        # Update tf-idf weight for word\n",
    "        \n",
    "        \n",
    "        # Reset number of comments containing a word\n",
    "        \n",
    "        \n",
    "        # Move onto next word in comment\n",
    "        \n",
    "        \n",
    "    return tfidf_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF(\"Check out my new music video plz\",data_comments_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
